#!/usr/bin/env bash
set -euo pipefail

PROJ="aws-eks-ecr-jenkins-demo"
echo "[+] Creating project: ${PROJ}"

# ------------------------------------------------------------------------------
# Helpers
# ------------------------------------------------------------------------------
write() {
  local path="$1"; shift
  mkdir -p "$(dirname "${PROJ}/${path}")"
  cat > "${PROJ}/${path}" <<'EOF'
'"$@"'
EOF
}

append() {
  local path="$1"; shift
  mkdir -p "$(dirname "${PROJ}/${path}")"
  cat >> "${PROJ}/${path}" <<'EOF'
'"$@"'
EOF
}

mkdir -p "${PROJ}"
mkdir -p "${PROJ}"/{terraform,helm,jenkins,scripts,app}
mkdir -p "${PROJ}"/helm/app-chart/{templates}
mkdir -p "${PROJ}"/helm/jenkins
mkdir -p "${PROJ}"/terraform/env/dev

# ------------------------------------------------------------------------------
# Terraform (modular: providers/backend/vars + vpc/ecr/iam/eks/outputs)
# ------------------------------------------------------------------------------

write terraform/providers.tf '
terraform {
  required_version = ">= 1.6.0"
  required_providers {
    aws = { source = "hashicorp/aws", version = "~> 5.0" }
    kubernetes = { source = "hashicorp/kubernetes", version = "~> 2.29" }
    helm = { source = "hashicorp/helm", version = "~> 2.13" }
  }
}

provider "aws" { region = var.aws_region }

# These are wired after cluster creation, safe for terraform validate
provider "kubernetes" {
  host                   = try(module.eks_out.cluster_endpoint, null)
  cluster_ca_certificate = try(base64decode(module.eks_out.cluster_ca), null)
  token                  = try(data.aws_eks_cluster_auth.this.token, null)
}

provider "helm" {
  kubernetes {
    host                   = try(module.eks_out.cluster_endpoint, null)
    cluster_ca_certificate = try(base64decode(module.eks_out.cluster_ca), null)
    token                  = try(data.aws_eks_cluster_auth.this.token, null)
  }
}
'

write terraform/backend.tf '
# Default to local backend for restricted environments.
terraform {
  backend "local" {
    path = "env/dev/terraform.tfstate"
  }
}

# --- If you want S3 backend, replace the above with:
# terraform {
#   backend "s3" {
#     bucket         = "YOUR-S3-BUCKET-NAME"
#     key            = "tfstate/demo/eks/terraform.tfstate"
#     region         = "ap-south-1"
#     dynamodb_table = "terraform-locks"   # optional if you use state locking
#     encrypt        = true
#   }
# }
'

write terraform/variables.tf '
variable "project_name"         { type = string, default = "demo" }
variable "aws_region"           { type = string, default = "ap-south-1" }
variable "vpc_cidr"             { type = string, default = "10.20.0.0/16" }
variable "public_subnets"       { type = list(string), default = ["10.20.1.0/24","10.20.2.0/24"] }
variable "private_subnets"      { type = list(string), default = ["10.20.3.0/24","10.20.4.0/24"] }
variable "eks_version"          { type = string, default = "1.30" }
variable "node_instance_type"   { type = string, default = "t3.medium" }
variable "desired_capacity"     { type = number, default = 2 }
variable "ecr_repo_name"        { type = string, default = "demo-app" }

variable "tags" {
  type = map(string)
  default = { project = "demo-eks-ecr" }
}
'

write terraform/env/dev/dev.tfvars '
project_name       = "demo"
aws_region         = "ap-south-1"
vpc_cidr           = "10.20.0.0/16"
public_subnets     = ["10.20.1.0/24", "10.20.2.0/24"]
private_subnets    = ["10.20.3.0/24", "10.20.4.0/24"]
eks_version        = "1.30"
node_instance_type = "t3.medium"
desired_capacity   = 2
ecr_repo_name      = "demo-app"
'

write terraform/vpc.tf '
resource "aws_vpc" "main" {
  cidr_block           = var.vpc_cidr
  enable_dns_support   = true
  enable_dns_hostnames = true
  tags = merge(var.tags, { Name = "${var.project_name}-vpc" })
}

data "aws_availability_zones" "available" {}

resource "aws_internet_gateway" "igw" {
  vpc_id = aws_vpc.main.id
  tags   = merge(var.tags, { Name = "${var.project_name}-igw" })
}

resource "aws_subnet" "public" {
  for_each = { a = var.public_subnets[0], b = var.public_subnets[1] }
  vpc_id                  = aws_vpc.main.id
  cidr_block              = each.value
  availability_zone       = data.aws_availability_zones.available.names[lookup({"a":0,"b":1}, each.key)]
  map_public_ip_on_launch = true
  tags = merge(var.tags, { Name = "${var.project_name}-public-${each.key}", "kubernetes.io/role/elb" = "1" })
}

resource "aws_route_table" "public" {
  vpc_id = aws_vpc.main.id
  tags   = merge(var.tags, { Name = "${var.project_name}-public-rt" })
}

resource "aws_route" "public_internet" {
  route_table_id         = aws_route_table.public.id
  destination_cidr_block = "0.0.0.0/0"
  gateway_id             = aws_internet_gateway.igw.id
}

resource "aws_route_table_association" "public_assoc" {
  for_each       = aws_subnet.public
  subnet_id      = each.value.id
  route_table_id = aws_route_table.public.id
}

resource "aws_subnet" "private" {
  for_each = { a = var.private_subnets[0], b = var.private_subnets[1] }
  vpc_id            = aws_vpc.main.id
  cidr_block        = each.value
  availability_zone = data.aws_availability_zones.available.names[lookup({"a":0,"b":1}, each.key)]
  tags = merge(var.tags, { Name = "${var.project_name}-private-${each.key}", "kubernetes.io/role/internal-elb" = "1" })
}

resource "aws_eip" "nat" { vpc = true tags = merge(var.tags, { Name = "${var.project_name}-nat-eip" }) }

resource "aws_nat_gateway" "nat" {
  allocation_id = aws_eip.nat.id
  subnet_id     = values(aws_subnet.public)[0].id
  tags          = merge(var.tags, { Name = "${var.project_name}-nat" })
}

resource "aws_route_table" "private" { vpc_id = aws_vpc.main.id tags = merge(var.tags, { Name = "${var.project_name}-private-rt" }) }

resource "aws_route" "private_nat" {
  route_table_id         = aws_route_table.private.id
  destination_cidr_block = "0.0.0.0/0"
  nat_gateway_id         = aws_nat_gateway.nat.id
}

resource "aws_route_table_association" "private_assoc" {
  for_each       = aws_subnet.private
  subnet_id      = each.value.id
  route_table_id = aws_route_table.private.id
}
'

write terraform/ecr.tf '
resource "aws_ecr_repository" "app" {
  name                 = var.ecr_repo_name
  image_tag_mutability = "MUTABLE"
  image_scanning_configuration { scan_on_push = true }
  tags = var.tags
}
'

write terraform/iam.tf '
data "aws_iam_policy_document" "eks_cluster_assume" {
  statement {
    actions = ["sts:AssumeRole"]
    principals { type = "Service" identifiers = ["eks.amazonaws.com"] }
  }
}

resource "aws_iam_role" "eks_cluster" {
  name               = "${var.project_name}-eks-cluster-role"
  assume_role_policy = data.aws_iam_policy_document.eks_cluster_assume.json
  tags               = var.tags
}

resource "aws_iam_role_policy_attachment" "eks_cluster_AmazonEKSClusterPolicy" {
  role       = aws_iam_role.eks_cluster.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSClusterPolicy"
}

data "aws_iam_policy_document" "eks_node_assume" {
  statement {
    actions = ["sts:AssumeRole"]
    principals { type = "Service" identifiers = ["ec2.amazonaws.com"] }
  }
}

resource "aws_iam_role" "eks_node" {
  name               = "${var.project_name}-eks-node-role"
  assume_role_policy = data.aws_iam_policy_document.eks_node_assume.json
  tags               = var.tags
}

resource "aws_iam_role_policy_attachment" "node_AmazonEKSWorkerNodePolicy" {
  role       = aws_iam_role.eks_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKSWorkerNodePolicy"
}
resource "aws_iam_role_policy_attachment" "node_AmazonEC2ContainerRegistryReadOnly" {
  role       = aws_iam_role.eks_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEC2ContainerRegistryReadOnly"
}
resource "aws_iam_role_policy_attachment" "node_AmazonEKS_CNI_Policy" {
  role       = aws_iam_role.eks_node.name
  policy_arn = "arn:aws:iam::aws:policy/AmazonEKS_CNI_Policy"
}
'

write terraform/eks.tf '
resource "aws_security_group" "eks_cluster" {
  name        = "${var.project_name}-eks-cluster-sg"
  description = "EKS cluster communication"
  vpc_id      = aws_vpc.main.id
  tags        = var.tags
}

resource "aws_eks_cluster" "this" {
  name     = "${var.project_name}-eks"
  role_arn = aws_iam_role.eks_cluster.arn
  version  = var.eks_version

  vpc_config {
    # Control plane is public; nodes run in private subnets.
    subnet_ids              = concat([for s in aws_subnet.public : s.id], [for s in aws_subnet.private : s.id])
    endpoint_public_access  = true
    endpoint_private_access = false
    security_group_ids      = [aws_security_group.eks_cluster.id]
  }

  depends_on = [aws_iam_role_policy_attachment.eks_cluster_AmazonEKSClusterPolicy]
  tags       = var.tags
}

resource "aws_eks_node_group" "default" {
  cluster_name    = aws_eks_cluster.this.name
  node_group_name = "${var.project_name}-ng"
  node_role_arn   = aws_iam_role.eks_node.arn
  subnet_ids      = [for s in aws_subnet.private : s.id]     # nodes in private subnets
  instance_types  = [var.node_instance_type]

  scaling_config { desired_size = var.desired_capacity min_size = 1 max_size = var.desired_capacity + 1 }
  update_config  { max_unavailable = 1 }

  depends_on = [
    aws_iam_role_policy_attachment.node_AmazonEKSWorkerNodePolicy,
    aws_iam_role_policy_attachment.node_AmazonEC2ContainerRegistryReadOnly,
    aws_iam_role_policy_attachment.node_AmazonEKS_CNI_Policy,
  ]
  tags = var.tags
}

# OIDC provider for IRSA (future add-ons)
resource "aws_iam_openid_connect_provider" "oidc" {
  url             = aws_eks_cluster.this.identity[0].oidc[0].issuer
  client_id_list  = ["sts.amazonaws.com"]
  thumbprint_list = ["9e99a48a9960b14926bb7f3b02e22da0afd10df6"]
}

data "aws_eks_cluster_auth" "this" { name = aws_eks_cluster.this.name }

# Simple module to expose clean outputs for providers
module "eks_out" {
  source = "./modules/eks-outputs"
  cluster_name = aws_eks_cluster.this.name
  cluster_endpoint = aws_eks_cluster.this.endpoint
  cluster_ca = aws_eks_cluster.this.certificate_authority[0].data
  oidc_provider_arn = aws_iam_openid_connect_provider.oidc.arn
}
'

mkdir -p "${PROJ}/terraform/modules/eks-outputs"
write terraform/modules/eks-outputs/outputs.tf '
variable "cluster_name" {}
variable "cluster_endpoint" {}
variable "cluster_ca" {}
variable "oidc_provider_arn" {}

output "cluster_name"  { value = var.cluster_name }
output "cluster_endpoint" { value = var.cluster_endpoint }
output "cluster_ca"    { value = var.cluster_ca }
output "oidc_provider_arn" { value = var.oidc_provider_arn }
'

write terraform/outputs.tf '
output "aws_region"     { value = var.aws_region }
output "cluster_name"   { value = module.eks_out.cluster_name }
output "cluster_endpoint" { value = module.eks_out.cluster_endpoint }
output "ecr_repository_url" { value = aws_ecr_repository.app.repository_url }
output "oidc_provider_arn"  { value = module.eks_out.oidc_provider_arn }
output "cluster_certificate_authority_data" { value = module.eks_out.cluster_ca }
'

# ------------------------------------------------------------------------------
# Sample App (Node) + Dockerfile + Helm chart
# ------------------------------------------------------------------------------
write app/server.js '
const http = require("http");
const port = process.env.PORT || 8080;
const version = process.env.APP_VERSION || "v0";

const srv = http.createServer((req, res) => {
  if (req.url === "/healthz") {
    res.writeHead(200, {"Content-Type":"application/json"});
    return res.end(JSON.stringify({status:"ok", version}));
  }
  res.writeHead(200, {"Content-Type":"text/plain"});
  res.end(`Hello from demo app ${version}\n`);
});
srv.listen(port, () => console.log("Listening on", port));
'

write app/package.json '
{
  "name": "demo-app",
  "version": "1.0.0",
  "main": "server.js",
  "license": "MIT",
  "scripts": { "start": "node server.js" },
  "dependencies": {}
}
'

write app/Dockerfile '
# syntax=docker/dockerfile:1
FROM node:20-alpine
WORKDIR /app
COPY package.json .
RUN npm install --only=production || true
COPY server.js .
ENV PORT=8080
EXPOSE 8080
CMD ["npm","start"]
'

write helm/app-chart/Chart.yaml '
apiVersion: v2
name: app-chart
description: Simple demo app
type: application
version: 0.1.0
appVersion: "1.0.0"
'

write helm/app-chart/values.yaml '
replicaCount: 2

image:
  repository: CHANGEME_ECR_URL/demo-app
  pullPolicy: IfNotPresent
  tag: "latest"

service:
  type: LoadBalancer
  port: 80
  targetPort: 8080

resources: {}
nodeSelector: {}
tolerations: []
affinity: []
'

write helm/app-chart/templates/_helpers.tpl '
{{- define "app-chart.name" -}}app{{- end -}}
{{- define "app-chart.fullname" -}}app{{- end -}}
'

write helm/app-chart/templates/deployment.yaml '
apiVersion: apps/v1
kind: Deployment
metadata:
  name: {{ include "app-chart.fullname" . }}
  labels: { app: {{ include "app-chart.name" . }} }
spec:
  replicas: {{ .Values.replicaCount }}
  selector:
    matchLabels: { app: {{ include "app-chart.name" . }} }
  strategy:
    type: RollingUpdate
    rollingUpdate: { maxUnavailable: 0, maxSurge: 1 }
  template:
    metadata:
      labels: { app: {{ include "app-chart.name" . }} }
    spec:
      containers:
        - name: app
          image: "{{ .Values.image.repository }}:{{ .Values.image.tag }}"
          imagePullPolicy: {{ .Values.image.pullPolicy }}
          ports: [ { containerPort: 8080 } ]
          env:
            - name: APP_VERSION
              value: "{{ .Values.image.tag }}"
          readinessProbe:
            httpGet: { path: /healthz, port: 8080 }
            initialDelaySeconds: 3
            periodSeconds: 5
          livenessProbe:
            httpGet: { path: /healthz, port: 8080 }
            initialDelaySeconds: 5
            periodSeconds: 10
'

write helm/app-chart/templates/service.yaml '
apiVersion: v1
kind: Service
metadata:
  name: {{ include "app-chart.fullname" . }}
  labels: { app: {{ include "app-chart.name" . }} }
spec:
  type: {{ .Values.service.type }}
  selector: { app: {{ include "app-chart.name" . }} }
  ports:
    - port: {{ .Values.service.port }}
      targetPort: {{ .Values.service.targetPort }}
'

# ------------------------------------------------------------------------------
# Jenkins via Helm (separate chart values)
#  - Uses the official jenkinsci/jenkins chart
#  - Service: LoadBalancer for quick access in practice env
# ------------------------------------------------------------------------------
write helm/jenkins/values.yaml '
controller:
  installPlugins:
    - kubernetes:4269.v1d05e5f52ca_d
    - workflow-aggregator:596.v8c21c963d92d
    - git:5.3.0
    - email-ext:2.104
    - mailer:489.vd4b_7b_a_42b_85a_
  adminUser: admin
  adminPassword: admin123   # change later
  jenkinsUriPrefix: ""
  jenkinsUrl: ""
  JCasC:
    defaultConfig: true
  serviceType: LoadBalancer
  resources:
    requests: { cpu: "200m", memory: "512Mi" }
    limits:   { cpu: "1",    memory: "2Gi" }

agent:
  enabled: true

persistence:
  enabled: true
  size: 8Gi
'

# ------------------------------------------------------------------------------
# Jenkins Pipeline (email notifications via Email Extension)
# ------------------------------------------------------------------------------
write jenkins/Jenkinsfile '
pipeline {
  agent any
  options { timestamps() }

  environment {
    AWS_REGION = "ap-south-1"
    ECR_REPO   = "demo-app"
    APP_NAME   = "app"
    APP_NS     = "default"
  }

  stages {
    stage("Checkout") { steps { checkout scm } }

    stage("Terraform Apply (idempotent)") {
      steps {
        sh """
          set -e
          cd terraform
          terraform init -input=false
          terraform apply -input=false -auto-approve -var-file=env/dev/dev.tfvars
        """
      }
    }

    stage("ECR Login") { steps { sh "scripts/ecr-login.sh" } }

    stage("Build & Push Image") {
      steps {
        script {
          def commit = sh(script: "git rev-parse --short HEAD", returnStdout: true).trim()
          def tag = "${commit}-${env.BUILD_NUMBER}"
          sh """
            set -e
            ACCOUNT_ID=\$(aws sts get-caller-identity --query Account --output text)
            REGISTRY="\${ACCOUNT_ID}.dkr.ecr.${AWS_REGION}.amazonaws.com"
            IMAGE="\${REGISTRY}/${ECR_REPO}:${tag}"
            docker build -t "\${IMAGE}" ./app
            docker push "\${IMAGE}"
            echo "\${IMAGE}" > image.txt
          """
        }
      }
    }

    stage("Deploy (Helm --atomic)") {
      steps {
        sh """
          set -e
          CLUSTER_NAME=\$(terraform -chdir=terraform output -raw cluster_name)
          aws eks update-kubeconfig --name "\${CLUSTER_NAME}" --region "${AWS_REGION}"

          IMAGE=\$(cat image.txt)
          REPO=\$(echo "\$IMAGE" | awk -F: '{print $1}')
          TAG=\$(echo  "\$IMAGE" | awk -F: '{print $2}')
          mkdir -p helm/app-chart/generated
          sed "s#CHANGEME_ECR_URL/demo-app#\${REPO%:*}#g" helm/app-chart/values.yaml > helm/app-chart/generated/values.yaml

          if command -v yq >/dev/null; then
            yq -i ".image.tag = \\"${TAG}\\"" helm/app-chart/generated/values.yaml
          else
            sed -i "s#tag:.*#tag: \\"${TAG}\\"#g" helm/app-chart/generated/values.yaml
          fi

          helm upgrade --install ${APP_NAME} helm/app-chart \
            -n ${APP_NS} --create-namespace \
            -f helm/app-chart/generated/values.yaml \
            --atomic --wait --timeout 5m
        """
      }
    }

    stage("Post-Deploy Validation") { steps { sh "scripts/validate.sh" } }
  }

  post {
    success {
      emailext(
        to: "you@example.com",
        subject: "SUCCESS: ${JOB_NAME} #${BUILD_NUMBER}",
        body: "Deployment succeeded. Job: ${JOB_NAME} #${BUILD_NUMBER}"
      )
    }
    failure {
      script { sh "scripts/rollback.sh || true" }
      emailext(
        to: "you@example.com",
        subject: "FAILURE: ${JOB_NAME} #${BUILD_NUMBER}",
        body: "Deployment failed. Rollback attempted. Check Jenkins logs."
      )
    }
  }
}
'

# ------------------------------------------------------------------------------
# Utility scripts
# ------------------------------------------------------------------------------
write scripts/ecr-login.sh '
#!/usr/bin/env bash
set -euo pipefail
AWS_REGION="${AWS_REGION:-ap-south-1}"
aws ecr get-login-password --region "${AWS_REGION}" \
  | docker login --username AWS --password-stdin \
    "$(aws sts get-caller-identity --query Account --output text).dkr.ecr.${AWS_REGION}.amazonaws.com"
'
write scripts/validate.sh '
#!/usr/bin/env bash
set -euo pipefail
APP_NS="${APP_NS:-default}"
APP_NAME="${APP_NAME:-app}"
TIMEOUT="${TIMEOUT:-180}"
echo "[validate] rollout status..."
kubectl rollout status deploy/${APP_NAME} -n ${APP_NS} --timeout=${TIMEOUT}s
TYPE=$(kubectl get svc ${APP_NAME} -n ${APP_NS} -o jsonpath="{.spec.type}")
if echo "$TYPE" | grep -qi LoadBalancer; then
  for i in {1..30}; do
    EP=$(kubectl get svc ${APP_NAME} -n ${APP_NS} -o jsonpath="{.status.loadBalancer.ingress[0].hostname}{.status.loadBalancer.ingress[0].ip}")
    [[ -n "$EP" ]] && break
    echo "Waiting for ELB... ($i/30)"; sleep 5
  done
  curl -sf "http://${EP}/healthz"
fi
echo "[validate] OK"
'
write scripts/rollback.sh '
#!/usr/bin/env bash
set -euo pipefail
APP_NS="${APP_NS:-default}"
APP_NAME="${APP_NAME:-app}"
REVISION="${REVISION:-0}"
echo "[rollback] helm rollback ${APP_NAME} ${REVISION} -n ${APP_NS}"
helm rollback ${APP_NAME} ${REVISION} -n ${APP_NS} --wait
kubectl rollout status deploy/${APP_NAME} -n ${APP_NS} --timeout=180s
echo "[rollback] done"
'
chmod +x "${PROJ}"/scripts/*.sh

# ------------------------------------------------------------------------------
# README (quick steps + Jenkins-on-EKS notes)
# ------------------------------------------------------------------------------
write README.md '
# AWS EKS + ECR CI/CD (Terraform + Helm + Jenkins)

## What gets created
- **VPC** (public & private subnets; NAT)
- **ECR** repo (`demo-app`)
- **EKS** 1.30: control plane public; **nodes in private subnets**
- **App Helm chart** (LB service, probes, atomic rollback)
- **Jenkins via Helm** (official chart), service = LoadBalancer
- **Jenkins pipeline**: Terraform → build&push → Helm deploy → validate → email

## 1) Provision infra
```bash
cd terraform
terraform init
terraform apply -auto-approve -var-file=env/dev/dev.tfvars
aws eks update-kubeconfig --region ap-south-1 --name $(terraform output -raw cluster_name)
